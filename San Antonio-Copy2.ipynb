{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Area\n",
    "\n",
    "San Antonio, TX United States\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/metro/san-antonio_texas/\n",
    "\n",
    "This map is of San Antonio, the city where my grandparents lived and where I spent the holidays growing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"san-antonio_texas.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 1 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map\n",
    "\n",
    "I noticed several problems with the data: \n",
    "\n",
    "Numerous abbreviations for street names ('North US Highway 281','US Highway 281','United States Highway 281')\n",
    "\n",
    "\n",
    "Inconsistent postal codes (“78155”, “78155-2214”)\n",
    "\n",
    "“Incorrect” postal codes (San Antonio area zip codes all begin with “72” however several zip codes were listed for \"282\" which corresponds to Charlotte, NC.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I queried the street names by using audit.py to identify problems with the street names.  I iterated over each word in an address and corrected them to a mapping list below using an update_name function:\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ste\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Hwy\": \"Interstate Highway\",\n",
    "            \"Hiwy\": \"Interstate Highway\",\n",
    "            \"IH\": \"Interstate Highway\",\n",
    "            \"I-\": \"Interstate Highway\",\n",
    "            \"I-H\": \"Interstate Highway\",\n",
    "            \"Interstate\": \"Interstate Highway\",\n",
    "            \"Interstate\": \"Interstate Highway\", \n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"FM\": \"Farm-to-Market\",\n",
    "            \"Plz\": \"plaza\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not clean postal codes, but further steps would be to limit postal codes to 5 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cur' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c91690b8e666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ORDER BY count DESC;'''\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQUERY_ZIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur' is not defined"
     ]
    }
   ],
   "source": [
    "QUERY_ZIP = '''SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags \n",
    "    UNION ALL \n",
    "    SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key='postcode'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC;'''\n",
    "cur.execute(QUERY_ZIP)\n",
    "result = cur.fetchall()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three zip codes 78155, 78666, and 78006 in the 10 of zip codes for San Antonio appear to be in suburbs outside the San Antonio \n",
    "1604 loop. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUERY_ZIP2 = '''SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags \n",
    "    UNION ALL \n",
    "    SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key='postcode'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC\n",
    "LIMIT 10;'''\n",
    "cur.execute(QUERY_ZIP2)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "We can see from the count tags function the amount of nodes, members, tags, and ways below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 23537,\n",
       " 'nd': 1479783,\n",
       " 'node': 1244193,\n",
       " 'osm': 1,\n",
       " 'relation': 1718,\n",
       " 'tag': 751039,\n",
       " 'way': 144603}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "\n",
    "    return tags\n",
    "\n",
    "count_tags(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if re.search(lower,k):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.search(lower_colon,k):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif re.search(problemchars,k):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "        return keys\n",
    "        pass\n",
    "        \n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 432074, 'lower_colon': 310783, 'other': 8182, 'problemchars': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_map(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "OSMFILE = \"example.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Interstate Highway\", \"Farm-to-Market\" ]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ste\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Hwy\": \"Interstate Highway\",\n",
    "            \"Hiwy\": \"Interstate Highway\",\n",
    "            \"IH\": \"Interstate Highway\",\n",
    "            \"I-\": \"Interstate Highway\",\n",
    "            \"I-H\": \"Interstate Highway\",\n",
    "            \"Interstate\": \"Interstate Highway\",\n",
    "            \"Interstate\": \"Interstate Highway\", \n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"FM\": \"Farm-to-Market\",\n",
    "            \"Plz\": \"plaza\"\n",
    "            }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            print \"BEFORE\"\n",
    "            print name\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "            print \"AFTER\"\n",
    "            print name\n",
    "            \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for attribute in element.attrib:\n",
    "            if attribute in NODE_FIELDS:\n",
    "                node_attribs[attribute]=element.attrib[attribute]\n",
    "        \n",
    "        sub_iter=element.iter(\"tag\")\n",
    "        for atr in sub_iter:\n",
    "            k_val=atr.attrib['k']\n",
    "            locol=LOWER_COLON.search(k_val)\n",
    "            prochar=PROBLEMCHARS.search(k_val)\n",
    "            if locol:\n",
    "                key_list = k_val.split(':',1)\n",
    "                k_key=key_list[1]\n",
    "                tag_type=key_list[0]\n",
    "                                    # use cleaning function:\n",
    "                if k_val == 'addr:street':\n",
    "                    k_key = update_name(k_val, mapping)\n",
    "                                     # otherwise:\n",
    "                else:\n",
    "                    k_key = k_val  \n",
    "            elif prochar:\n",
    "                continue\n",
    "            else:\n",
    "                tag_type=\"regular\"\n",
    "                k_key=k_val\n",
    "            v_val=atr.attrib['v']\n",
    "            content={\"id\":node_attribs['id'],'key':k_key,'value':v_val,'type':tag_type}\n",
    "            tags.append(content)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for attribute in element.attrib:\n",
    "            if attribute in WAY_FIELDS:\n",
    "                way_attribs[attribute]=element.attrib[attribute]\n",
    "    \n",
    "        sub_iter=element.iter(\"nd\")\n",
    "        level=0\n",
    "        for atr in sub_iter:\n",
    "            for sub_attrib in atr.attrib:\n",
    "                if sub_attrib=='ref':\n",
    "                    content= {\"id\":way_attribs['id'],'node_id':atr.attrib[sub_attrib],'position':level}\n",
    "                    way_nodes.append(content)\n",
    "                    level+=1\n",
    "        sub_iter=element.iter(\"tag\")\n",
    "        for atr in sub_iter:\n",
    "            k_val=atr.attrib['k']\n",
    "            locol=LOWER_COLON.search(k_val)\n",
    "            prochar=PROBLEMCHARS.search(k_val)\n",
    "            if locol:\n",
    "                key_list = k_val.split(':',1)\n",
    "                k_key=key_list[1]\n",
    "                tag_type=key_list[0]\n",
    "                    # use cleaning function:\n",
    "                if k_val == 'addr:street':\n",
    "                    k_key = update_name(k_val, mapping)\n",
    "                    # otherwise:\n",
    "                else:\n",
    "                    k_key = k_val  \n",
    "            elif prochar:\n",
    "                continue\n",
    "            else:\n",
    "                tag_type=\"regular\"\n",
    "                k_key=k_val\n",
    "            v_val=atr.attrib['v']\n",
    "            content={\"id\":way_attribs['id'],'key':k_key,'value':v_val,'type':tag_type}\n",
    "            tags.append(content)            \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Data to SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add nodes_tags table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite_file = 'sql_db.db'    # name of the sqlite database file\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(sqlite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a cursor object\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ways_tags table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE ways_tags(id INTEGER, key TEXT, value TEXT,type TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add ways_nodes table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "X NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "X WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "X WAY_NODES_FIELDS = ['id', 'node_id', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE ways_nodes(id INTEGER, node_id INTEGER, position INTEGER)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['node_id'].decode(\"utf-8\"), i['position'].decode(\"utf-8\")) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add ways table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE ways(id INTEGER, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"),\n",
    "              i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add nodes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "    CREATE TABLE nodes(id INTEGER, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT)\n",
    "''')\n",
    "# commit the changes\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['lat'].decode(\"utf-8\"), i['lon'].decode(\"utf-8\"),\n",
    "              i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"),\n",
    "              i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert the formatted data\n",
    "cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "# commit the changes\n",
    "conn.commit()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "from hurry.filesize import size\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "\n",
    "for filename, size in files_list:\n",
    "    print '{:.<40s}: {:5s}'.format(filename,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries below show that there are 1,244,193 nodes and 144,603 ways in the SQL table.  These amounts tie to count tag functon that I used prior to importing the data set into the SQL table.  This query is a check to help ensure that we have uploaded all the data from the csv file correctly to the SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_file\n",
    "\n",
    "# Connecting to the database file\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "cur = conn.cursor()\n",
    "\n",
    "QUERY1 = '''SELECT COUNT(*) \n",
    "FROM nodes'''\n",
    "\n",
    "cur.execute(QUERY1)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERY2 = '''SELECT COUNT(*) \n",
    "FROM ways'''\n",
    "\n",
    "cur.execute(QUERY2)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique users is 828."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERY3 = '''SELECT COUNT(DISTINCT(e.uid))\n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e\n",
    "'''\n",
    "\n",
    "cur.execute(QUERY3)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below shows the number of top 10 amenties in the area.  Places of worship, schools, and restaurants round out the top 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "QUERY4 = '''SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;'''\n",
    "\n",
    "cur.execute(QUERY4)\n",
    "result = cur.fetchall()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that burger restaurants are the most prevalent in San Antonio.  I would have guessed Mexican restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERY5 = '''SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='fast_food') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='cuisine'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;'''\n",
    "\n",
    "cur.execute(QUERY5)\n",
    "result = cur.fetchall()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "QUERY6 = '''SELECT e.user, COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC;'''\n",
    "\n",
    "cur.execute(QUERY6)\n",
    "result_user = cur.fetchall()\n",
    "print(result_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(result_user)\n",
    "df = df.rename(columns={0: 'User', 1: 'Count'})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 828 users in the San Antonio data set.  However, the data seems to be skewed by super users.  For example, the top 4 users all have over 100k contributions each and account for over 1.1M contributions.  I believe these users are skewing the data set and should be removed from any further data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(df.index[[0,1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean drops significantly from 1677 to 349 contributions per user just by removing the top four users from the data set.  However, there still appears to be a major drop off in user contributions with 50% of the users only making 2 contributions.  Additionally, the 75th percentile is at 74 contributions per user. Motivating the 50th and 75th percentile user base should be a next step to continually improve the accuracy of the San Antonio area.\n",
    "\n",
    "One possible way to increase involvement is to use gamification.  For example, Treehouse is a virtual training academy for learning code, app development, and business skills, used by beginners to learn valuable career skills and experienced professionals for career advancement. Students choose from various tracks for defined outcomes, and earn badges and points as they work through the library of courses to show off their achievements and impress potential employers. Tracks are broken into manageable chunks, and a tracker shows your progress as you work towards your goals. The more points you earn, the higher your potential salary.\n",
    "\n",
    "Solution\n",
    "Benefits\n",
    "Benefit #1\n",
    "Benefit #2\n",
    "Anticipated Issues\n",
    "How long will this be sustainable?\n",
    "Version control? - Overwriting others contributions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
